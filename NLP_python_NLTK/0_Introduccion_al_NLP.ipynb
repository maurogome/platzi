{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fundamentos de Procesamiento de Lenguaje Natural con Python y NLTK](https://platzi.com/clases/python-lenguaje-natural/)\n",
    "\n",
    "# Introducción al Procesamiento de Lenguaje Natural\n",
    "\n",
    "**NLP:** Natural Language Processing\n",
    "\n",
    "**NLU:** Natural Language Understanding\n",
    "\n",
    "**Test de Turing:** Si un humano no puede distingiuie entre una máquina y una persona en **una conversación**, entonces esa máquina ha alcanzado un nivel de inteligencia comparable al de un humano...\n",
    "\n",
    "---\n",
    "\n",
    "## Usos actuales del NLP\n",
    "- Máuinas de búsqueda\n",
    "- Traducción de texto\n",
    "- Chatbots\n",
    "- Análisis de discurso\n",
    "- Reconocimiento del habla\n",
    "\n",
    "El lenguaje humano es difuso, ambiguo y requiere mucho contexto\n",
    "\n",
    "## Evolución del NLP\n",
    "- 1950s-1990s: Sistemas basados en reglas\n",
    "- 1990s-2000s: Estadística de Corpus\n",
    "- 2000s-2014s: Machine Learning\n",
    "- 2014s-2021?: Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Avances de NLP\n",
    "\n",
    "> Entendimiento de texto (Bajo nivel)\n",
    ">> - Morfologia\n",
    ">>\n",
    ">> - Sintaxis\n",
    ">>\n",
    ">> - Semántica\n",
    ">\n",
    "> Aprendizaje de Representaciones\n",
    ">> - Vectores de Palabras\n",
    ">>\n",
    ">> - Verctores de Frases\n",
    ">>\n",
    ">> - Mecanismos de Atención\n",
    "\n",
    "- Redes LSTM\n",
    "- BiLSTM\n",
    "- RedesTransformer\n",
    "- Redes Reformer\n",
    "\n",
    "### Librerias:\n",
    "- NLTK\n",
    "- spaCy\n",
    "\n",
    "# Conceptos básicos de NLP\n",
    "\n",
    "*El lenguaje consiste (en sentido linguistico) en entender y caracterizar las reglas que determinan cómo estructurar expresiones linguisticas...*\n",
    "\n",
    "### Lenguaje como objeto de estudio\n",
    "\n",
    "- NLP: Se enfoca a aplicaciones prácticas en la ingenieria\n",
    "- Linguistica computacional (LC): Se enfoca en responder cuestiones del lenguaje con un fin puramente científico ¿Qué y cómo computan las personas?. La LC tiene dos tipo de modelos, los modelos basados en conocimiento (Knowladge Based)  y los modelos basados en datos (Data Driven)\n",
    "\n",
    "## Normalización de Texto\n",
    "Consiste en varios procesos de limpieza y transformación: **Tokenización, Lematización y Segmentación** a nivel de frases o sentencias.\n",
    "\n",
    "- **Tokenización:** Consiste en separar una frase en tokens, o en unidades mínimas linguisticas (asociadas normalmente con palabras)\n",
    "- **Lematización:** Convertir cada una de los tokens en su raíz fundamental.\n",
    "- **Segmentación:** Separar en frases, normalemente podemos usar las comas (,) para esto. El problema es que no siempre se aplica.\n",
    "\n",
    "\n",
    "*Mi hermano dejo de comer*\n",
    "\n",
    "**Tokenización:**\n",
    "\n",
    "| Mi | hermano | dejó | de | comer |\n",
    "\n",
    "**Lematización:**\n",
    "\n",
    "| Mi | hermano | **dejar** | de | comer |\n",
    "\n",
    "**Segmentación:**\n",
    "\n",
    "Frase: *Mi hermano dejo de comer, por eso no se siente bien.*\n",
    "\n",
    "> Mi hermano dejo de comer\n",
    ">\n",
    "> Por eso no se siente bien\n",
    "\n",
    "---\n",
    "\n",
    "**Corpus:** Colección de texto.\n",
    "\n",
    "**Corpora:** Colección de colecciones de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\cess_esp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importart Librerias y descargar Corpus\n",
    "import nltk\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expresiones Regulares\n",
    "\n",
    "[Curso de Expresiones Regulares](https://platzi.com/clases/expresiones-regulares/)\n",
    "\n",
    "Las expresiones regulares constituyen un lenguaje estandarizado para definir cadenas de busqueda de texto o patrones para buscar cadenas de texto en cuerpos de texto.\n",
    "\n",
    "[Expresiones Regulares en Python: re](https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
      "6030\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus = nltk.corpus.cess_esp.sents()\n",
    "print(corpus)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192686\n"
     ]
    }
   ],
   "source": [
    "# aplanamos la lista concatenado todas las sublistas en una lista grande\n",
    "# de forma que ya no tenemos una lista de listas, sino uno sola lista\n",
    "# con todos los titulares tokenizados uno tras otro\n",
    "\n",
    "flatten = [w for l in corpus for w in l]\n",
    "print(len(flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.', 'Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.', 'La', 'electricidad', 'producida', 'pasará', 'a', 'la', 'red', 'eléctrica', 'pública', 'de', 'México', 'en_virtud_de', 'un', 'acuerdo', 'de', 'venta']\n"
     ]
    }
   ],
   "source": [
    "# Miremos los primeros 100 tokens de la lista\n",
    "print(flatten[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras, textos y vocabularios\n",
    "\n",
    "### Estructura de la función re.search()\n",
    "\n",
    "Determina si el patrón de búsqueda **p** esta contenido en la cadena **s**\n",
    "\n",
    "re.search(p, s)\n",
    "\n",
    "**p** representa una expresión regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [w for w in flatten if re.search('es', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']\n"
     ]
    }
   ],
   "source": [
    "print(arr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El patrón de búsqueda lo que hace es verificar si dentro de la palabra que esta iterando aparece la expresión 'es' sin importar donde aparezca. A esta expresión regular es lo que llamamos *metacaracter básico*\n",
    "\n",
    "**Metacaracter:** Es una cadena de texto que define un patrón de búsqueda muy básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jueves', 'centrales', 'millones', 'millones', 'dólares']\n"
     ]
    }
   ],
   "source": [
    "arr = [w for w in flatten if re.search('es$', w)]\n",
    "print(arr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al incluir el simbolo '$' al final de la expresión estamos indicando que estamos buscando todas las cadenas finalizadas con la expresión 'es'\n",
    "\n",
    "Por el contrario, si queremos que la cadena empice con la expresión 'es', entonces debemos incluir el simbolo '^' al inicio de la expresión, asi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estatal', 'es', 'esta', 'esta', 'eso']\n"
     ]
    }
   ],
   "source": [
    "arr = [w for w in flatten if re.search('^es', w)]\n",
    "print(arr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busqueda de expresiones regulares usando el concepto de **Rango**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grupo', 'hoy', 'gas', 'gas', 'intervendrá']\n"
     ]
    }
   ],
   "source": [
    "# Rango [a-z] : El rango es cualquier caractér entre la a y la z\n",
    "# Rango [ghi] : El rango es cualquier caractér entre g, h, i\n",
    "\n",
    "arr = [w for w in flatten if re.search('^[ghi]', w)]\n",
    "print(arr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clausuras\n",
    "\n",
    "Son expresiones que nos indican que una cadena de caractéres se puede repetir, así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-']\n"
     ]
    }
   ],
   "source": [
    "# * repetir 0 o más veces\n",
    "# + repetir 1 o más veces\n",
    "\n",
    "arr = [w for w in flatten if re.search('^(no)*', w)]\n",
    "print(arr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, hay palabras que no empiezan con la expresión 'no' por que le estamos pidiendo que se repita la expresión **cero** o más veces.\n",
    "\n",
    "Ahora veamos si le pedimos que se repita la expresión **una** o más veecs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['norte', 'no', 'no', 'noche', 'no']\n"
     ]
    }
   ],
   "source": [
    "arr = [w for w in flatten if re.search('^(no)+', w)]\n",
    "print(arr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de Expresiones Regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esta es \n",
      " una prueba\n"
     ]
    }
   ],
   "source": [
    "print('esta es \\n una prueba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que python entienda toda la cadena como texto plano *(raw)*, debemos incluir en el *print* la letra r antes de la cadena, así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esta es \\n una prueba\n"
     ]
    }
   ],
   "source": [
    "print(r'esta es \\n una prueba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización:\n",
    "\n",
    "Es el proceso mediante el cual se sub-divide una cadena de texto en unidades linguísticas minimas (palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\" La gente tiene estrellas pero no significan lo mismo para todos. \n",
    "            Para algunos, los que viajan, las estrellas son sus guías. \n",
    "            Para otros sólo son lucecitas. Para los sabios las estrellas \n",
    "            son motivo de estudio y para mi hombre de negocios, eran oro.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'La', 'gente', 'tiene', 'estrellas', 'pero', 'no', 'significan', 'lo', 'mismo', 'para', 'todos.', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'Para', 'algunos,', 'los', 'que', 'viajan,', 'las', 'estrellas', 'son', 'sus', 'guías.', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'Para', 'otros', 'sólo', 'son', 'lucecitas.', 'Para', 'los', 'sabios', 'las', 'estrellas', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'son', 'motivo', 'de', 'estudio', 'y', 'para', 'mi', 'hombre', 'de', 'negocios,', 'eran', 'oro.']\n"
     ]
    }
   ],
   "source": [
    "# Caso 1: tokenizar por espacios vacios\n",
    "\n",
    "print(re.split(r' ', texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'La', 'gente', 'tiene', 'estrellas', 'pero', 'no', 'significan', 'lo', 'mismo', 'para', 'todos.', 'Para', 'algunos,', 'los', 'que', 'viajan,', 'las', 'estrellas', 'son', 'sus', 'guías.', 'Para', 'otros', 'sólo', 'son', 'lucecitas.', 'Para', 'los', 'sabios', 'las', 'estrellas', 'son', 'motivo', 'de', 'estudio', 'y', 'para', 'mi', 'hombre', 'de', 'negocios,', 'eran', 'oro.']\n"
     ]
    }
   ],
   "source": [
    "# Caso 2: Tokenizacion usando regex\n",
    "\n",
    "print(re.split(r'[ \\t\\n]+', texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'La', 'gente', 'tiene', 'estrellas', 'pero', 'no', 'significan', 'lo', 'mismo', 'para', 'todos', 'Para', 'algunos', 'los', 'que', 'viajan', 'las', 'estrellas', 'son', 'sus', 'guías', 'Para', 'otros', 'sólo', 'son', 'lucecitas', 'Para', 'los', 'sabios', 'las', 'estrellas', 'son', 'motivo', 'de', 'estudio', 'y', 'para', 'mi', 'hombre', 'de', 'negocios', 'eran', 'oro', '']\n"
     ]
    }
   ],
   "source": [
    "# Caso 3: incluimos la expresión \\W la cual selecciona algunos \n",
    "# caracteres especiales\n",
    "\n",
    "print(re.split(r'[ \\W\\t\\n]+', texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En', 'loe', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
     ]
    }
   ],
   "source": [
    "texto = 'En loe E.U. esa postal vale $15.50...'\n",
    "print(re.split(r'[ \\W\\t\\n]+', texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En', 'loe', 'E.U.', 'esa', 'postal', 'vale', '$15.50', '...']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'''(?x)                  # Flag para iniciar el modo verbose\n",
    "              (?:[A-Z]\\.)+          # Hace match con abreviaciones como U.S.A.\n",
    "              | \\w+(?:-\\w+)*        # Hace match con palabras que pueden tener un guión interno\n",
    "              | \\$?\\d+(?:\\.\\d+)?%?  # Hace match con dinero o porcentajes como $15.5 o 100%\n",
    "              | \\.\\.\\.              # Hace match con puntos suspensivos\n",
    "              | [][.,;\"'?():-_`]    # Hace match con signos de puntuación\n",
    "'''\n",
    "\n",
    "nltk.regexp_tokenize(texto, pattern)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
